orbs:
  python: circleci/python@2.0.3
version: 2.1
jobs:
  run-tests:
    environment:
      RELEASE_NAME: example-release
    machine:
      image: ubuntu-2004:202107-02
    resource_class: 2xlarge
    steps:
    - checkout
    - run:
        name: Install Kind
        command: |
          curl -Lo ./kind https://kind.sigs.k8s.io/dl/v0.11.1/kind-linux-amd64
          chmod +x ./kind
          sudo mv ./kind /usr/local/bin/
    - run:
       name: create kind cluster
       command: |
         kind create cluster --image kindest/node:v1.21.1

    - run:
       name: Install jq
       command: |
         sudo apt-get update
         sudo apt-get install -y jq

    - run:
        name: Install kubectl
        command: |
          curl -Lo kubectl https://storage.googleapis.com/kubernetes-release/release/v1.12.0/bin/linux/amd64/kubectl
          chmod +x kubectl
          sudo mv kubectl /usr/local/bin/

    - run:
        name: docker custom build
        command: |
           cat <<EOM > Dockerfile
           FROM quay.io/astronomer/astro-runtime-dev:10.0.0-alpha2
           EOM
           docker build --pull --tag my-dags:0.0.1 .
           kind load docker-image my-dags:0.0.1

    - run:
        name: install helm
        command: |
          curl -fsSL -o get_helm.sh https://raw.githubusercontent.com/helm/helm/main/scripts/get-helm-3
          chmod +x get_helm.sh
          ./get_helm.sh
          helm repo add apache-airflow https://airflow.apache.org
          helm repo update

    - run:
       name: Install airflow
       command: |
          helm install $RELEASE_NAME apache-airflow/airflow -f ./values.yaml

    - run:
        name: verify airflow installed
        command: |
            kubectl get pods
            POD_NAME=($(kubectl get pods | grep -E 'worker' | awk '{print $1}'))
            echo "Pod Name: $POD_NAME"
            kubectl exec -it  $POD_NAME -- airflow version
            sleep 540s 
            kubectl exec -it  $POD_NAME -- airflow dags list
            kubectl exec -it  $POD_NAME -- airflow dags unpause throughput_dag
            output=$(kubectl exec -it  $POD_NAME -- airflow dags trigger throughput_dag -o json | awk 'NR==2' | sed 's/\x1B\[[0-9;]*[mGK]//g')  
            echo "output is : $output"
            execution_date=$(echo $output | jq -r '.[0].logical_date')
            echo "execution_date: $execution_date"
            # Add a loop to check the DAG state until it becomes 'success' or 'failed'
            kubectl exec -it $POD_NAME -- airflow dags state throughput_dag $execution_date
            while true; do
              state=$(kubectl exec -it $POD_NAME -- airflow dags state throughput_dag $execution_date |  awk '{print $1}' | sed 's/\x1B\[[0-9;]*[mGK]//g')
              state="${state#"${state%%[![:space:]]*}"}"
              state="${state%"${state##*[![:space:]]}"}"
              echo "state is $state"
              
              if [ "$state" == "success" ]; then
                 echo "DAG state is $state."
                 exit 0
              elif [ "$state" == "failed" ]; then
                 echo "DAG state is $state."
                 exit 1
              else
                 echo "state: $state"
                 sleep 300  # You can adjust the sleep interval as needed
              fi
            done
          
#
#    - run:
#       name: upgrade airflow
#       command: |
#          helm upgrade $RELEASE_NAME apache-airflow/airflow -f ./values.yaml --set defaultAirflowTag=2.7.3rc1
#          sleep 300s
#
#
#    - run:
#       name: verify airflow installed after upgrade
#       command: |
#            kubectl get pods
#            POD_NAME=($(kubectl get pods | grep -E 'worker' | awk '{print $1}'))
#            echo "Pod Name: $POD_NAME"
#            kubectl exec -it  $POD_NAME -- airflow version
#            sleep 180s
#            kubectl exec -it  $POD_NAME -- airflow dags list
#            kubectl exec -it  $POD_NAME -- airflow dags unpause init_dag
#            output=$(kubectl exec -it  $POD_NAME -- airflow dags trigger throughput_dag -o json | awk 'NR==2' | sed 's/\x1B\[[0-9;]*[mGK]//g')
#            echo "output is : $output"
#            execution_date=$(echo $output | jq -r '.[0].logical_date')
#            echo "execution_date: $execution_date"
#            # Add a loop to check the DAG state until it becomes 'success' or 'failed'
#            kubectl exec -it $POD_NAME -- airflow dags state init_dag $execution_date
#            while true; do
#              state=$(kubectl exec -it $POD_NAME -- airflow dags state init_dag $execution_date |  awk '{print $1}' | sed 's/\x1B\[[0-9;]*[mGK]//g')
#              state="${state#"${state%%[![:space:]]*}"}"
#              state="${state%"${state##*[![:space:]]}"}"
#              echo "state is $state"
#
#              if [ "$state" == "success" ]; then
#                 echo "DAG state is $state."
#                 exit 0
#              elif [ "$state" == "failed" ]; then
#                 echo "DAG state is $state."
#                 exit 1
#              else
#                 echo "state: $state"
#                 sleep 300  # You can adjust the sleep interval as needed
#              fi
#            done
#
#
#
              
             
             

workflows:
  turbulence-ci:
    jobs:
    - run-tests:
        context:
        - turbulence
        name: setup_airflow_env

